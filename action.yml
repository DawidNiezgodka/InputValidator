name: 'The name of your action here'
description: 'Provide a description here'
author: 'Your name or organization here'

inputs:
  bench_dir:
    description: |
      If the benchmark-related files and folders are in a subdirectory of the repository,
      please specify its name here. If the value is specified, the assumption will be made
      that the directory the value points to contains both ansible and terraform folders.
      The main idea behind this input is that you don't have to prepend all the paths
      with the directory name.
      If you don't want this behaviour, you can use the snr_directory and deploy_directory inputs.
    required: false
  number_of_metrics_to_evaluate:
    description: |
      The number of metrics that will be evaluated
    required: false
  #####
  # Deployment inputs
  #####
  deploy_instance_meta_public_key:
    description: |
      The public key that will be added to the instance.
      The private part will be used to enable SSH access to the instance 
      and execute Ansible commands (the private key must be set using secrets).
    required: true
  deploy_custom_inventory_file_path:
    description: |
      Whether to use custom Terraform inventory file.
    required: false
    default: 'terraform/templates/hosts.tpl'
  deploy_vars_file_path:
    description: |
      Path to Terraform variables file (.tfvars).
    required: true
  deploy_variables:
    description: A multiline string of Terraform variables (key-value pairs).
    required: false
    default: ''
  deploy_destroy_after_run:
    description: |
      Whether to destroy the infrastructure after the benchmark is finished.
      If this input is set to true, the infrastructure will be destroyed
      regardless of the benchmark result.
    required: false
    default: false
  deploy_lock_id:
    description: |
      If you received an error message that the state file is locked,
      you can use this input to unlock it.
      Please provide the lock id that was given to you in the error message.
    required: false
  deploy_extra_file_uploaded_name:
    description: |
      Extra file for deploy step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
      This input related to the name of the uploaded artifact.
    required: false
  deploy_extra_file_name:
    description: |
      Extra file for deploy step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
    required: false
  deploy_extra_file_target_path:
    description: |
      Indicates where the extra file shall be moved.
    required: false

  #####
  # Setup and run inputs
  #####

  snr_existing_config_path:
    description: |
      Path to the existing Ansible configuration file.
      If this input is provided,
      the default config won't be created 
      and your configuration will be added to the ANSIBLE_CONFIG env variable.
    required: false
  snr_def_cfg_dir:
    description: |
      Path to the directory where the default Ansible configuration will be created.
    required: false
    default: "ansible"
  snr_def_cfg_file_name:
    description: |
      The name of the default Ansible configuration file.
    required: false
    default: "ansible.cfg"
  snr_def_cfg_inventory_file_path:
    description: |
      The default inventory file path for Ansible.
    required: false
    default: "hosts.cfg"
  snr_def_cfg_privilege_escalation:
    description: |
      The default privilege escalation for Ansible.
    required: false
  snr_def_cfg_private_key_file_path:
    description: |
      The private key file path for Ansible that will be added to the default configuration.
  snr_def_cfg_host_checking:
    description: |
      The default host checking for Ansible.
    required: false
  snr_directory:
    description: |
      Relative path to the Ansible directory.
    required: false
    default: "ansible"
  snr_playbook_directory:
    description: |
      Relative path to the directory where playbooks that represent
      different benchmark phases are stored.
    required: false
    default: "playbooks"
  snr_execution_order:
    description: |
      The order in which the framework will execute the Ansible playbooks.
    required: true
  snr_requirements:
    description: |
      Path to Ansible requirements file.
    required: false
  snr_extra_options_string:
    description: |
      A list of extra options to be passed to Ansible.
      Please read the docs to learn more about the format
      and the way of escaping secrets and environment variables.
    required: false
  snr_extra_options_file:
    description: |
      A .yml file with extra options to be passed to Ansible.
      Please read the docs to learn more about the format
      and the way of escaping secrets and environment variables.
    required: false
  snr_extra_file_uploaded_name:
    description: |
      Extra file for the setup and run step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
      This input related to the name of the uploaded artifact.
    required: false
  snr_extra_file_name:
    description: |
      Extra file for the setup and run step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
    required: false
  snr_extra_file_target_path:
    description: |
      Indicates where the extra file shall be moved.
    required: false

  #####
  # Evaluation inputs
  #####

  eval_bench_group_name:
    description: |
      The name of a specific benchmark. The name will be appended to the data where all
      the benchmark results are stored. The name can be used to change graphs and compare results.
    required: false
    default: 'Benchmark'
  eval_metrics_to_evaluate:
    description: |
      The metrics that will be evaluated.
    required: false
  eval_branch_with_bench_data:
    description: |
      The name of the branch that contains the benchmark data.
    required: false
    default: 'bench_data'
  eval_previous_data_storage_folder:
    description: |
      The first use of this variable is in step called "Checkout the branch with previous bench res to a given folder".
      Thus, it is used to specify the folder where the previous benchmark data will be checked out.
      Afterward, in the logic of the BenchEval action, the folder and the file (file_with_previous_bench_data)
      are used to specify the location where the current benchmark data will be appended 
      so that it can be saved by executing the step "Save the current benchmark to the file with all bench data".
    required: false
    default: 'bench_data'
  eval_file_with_previous_bench_data:
    description: |
      The name of the file that contains the previous benchmark data.
      This file will be also used to append the current benchmark data.
      See the description of the previous_data_storage_folder variable for more details.
    required: false
    default: 'data.json'
  eval_bucket_results_folder_path:
    description: |
      The path to the GCP bucket folder that contains the benchmark results.
    required: false
  eval_bucket_result_file_path:
    description: |
      The path to the file that contains the benchmark result in the GCP bucket.
    required: false
  eval_local_download_path_for_results:
    description: |
      The path to the local folder where the benchmark results will be downloaded.
    required: false
    default: 'results'
  eval_save_current_bench_res:
    description: |
      Save the benchmark data pointed to by current_bench_res_file
      to a file located in benchmark_data_file'.
    required: false
    default: 'true'
  eval_add_action_page_job_summary:
    description: Leave a job summary with benchmark result comparison in the actions tab of a repo.
    required: false
    default: "on"
  eval_link_to_templated_gh_page_with_results:
    description: |
      The link to the GitHub page with the templated benchmark results.
    required: false
  eval_comparison_margins:
    description: |
      Describes by how much percentage a given value can deviate from the desired
      value (threshold or prev) to still qualify the benchmark as successful.
      If you have selected range as one of the modes in the above set of modes,
      and it is not the only function, please provide values of -1 for the remaining functions.
      For example, if you previously entered: smaller, smaller, range, and your range is 20%,
      this input should look as follows: "-1, -1, 20"
  eval_failing_condition:
    description: |
    required: false
    default: "any"
  eval_bench_group_to_compare:
    description: |
      The name of the benchmark that will be compared to the current benchmark.
    required: false
  eval_action_page_job_summary:
    description: Leave a short summary on the workflow page. Possible values are on, off, if_failed.
    required: false
    default: "on"
  eval_evaluation_method:
    description: |
      The method that will be used to evaluate the benchmark.
      The possible values are 'previous', 'previous_successful', 'threshold', 'jump_detection',
      'trend_detection_moving_ave', 'threshold_range',
      'trend_detection_deltas'
    required: true
  eval_threshold_values:
    description: Comma-separated list of threshold values for comparison.
    required: false
  eval_comparison_operators:
    description: |
      Comma-separated list of operators for threshold comparison. The possible values are smaller, bigger, and tolerance.
    required: false
  eval_alert_users_if_bench_failed:
    description: |
      Commas-separated list of users to alert if the benchmark failed.
    required: false
  eval_comment_to_commit:
    description: |
      Add commit comment with the detailed benchmark info.
      Possible values are on, off, if_failed.
    required: false
    default: "if_failed"
  eval_threshold_upper:
    description: Comma-separated list of upper threshold values for range comparison.
    required: false
  eval_threshold_lower:
    description: Comma-separated list of lower threshold values for range comparison.
    required: false
  eval_jump_detection_thresholds:
    description: The threshold values for jump detection.
    required: false
  eval_trend_thresholds:
    description: The threshold value for both deltas trend detection and moving average.
    required: false
  eval_moving_ave_window_size:
    description: The window size for moving average trend detection.
    required: false
  eval_trend_det_no_sufficient_data_strategy:
    description: |
      The strategy to use when there is not enough data to perform trend detection.
      The possible values are use_available and fail.
    required: false
    default: "use_available"
  eval_trend_det_successful_release_branch:
    description: |
      The name of the branch that contains the successful benchmark result
      in a release branch.
      If not specified, the main branch will be used.
    required: false
    default: "main"
  eval_extra_file_uploaded_name:
    description: |
      Extra file for the eval step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
      This input related to the name of the uploaded artifact.
    required: false
  eval_extra_file_name:
    description: |
      Extra file for the eval step.
      The file will be downloaded to the root of the repository
      using download-artifact action.
    required: false
  eval_extra_file_target_path:
    description: |
      Indicates where the extra file shall be moved.
    required: false
  eval_result_files_merge_strategy_for_each_metric:
    description: |
      The strategy to use when merging the the benchmark results from the downloaded folder.
      This should be a comma separated list of strategies. Each strategy corresponds to a metric.
      The possible values are: sum, average, max, min, median.
    required: false

outputs:
  time:
    description: 'Your output description here'

runs:
  using: node20
  main: dist/index.js
